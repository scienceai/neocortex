{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU, ParametricSoftplus\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 3, 32, 32)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "nb_classes = 10\n",
    "nb_epoch = 100\n",
    "data_augmentation = False\n",
    "\n",
    "# shape of the image (SHAPE x SHAPE)\n",
    "shapex, shapey = 32, 32\n",
    "# number of convolutional filters to use at each layer\n",
    "nb_filters = [32, 64]\n",
    "# level of pooling to perform at each layer (POOL x POOL)\n",
    "nb_pool = [2, 2]\n",
    "# level of convolution to perform at each layer (CONV x CONV)\n",
    "nb_conv = [3, 3]\n",
    "# the CIFAR10 images are RGB\n",
    "image_dimensions = 3\n",
    "\n",
    "# the data, shuffled and split between tran and test sets\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation or normalization\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 28s - loss: 1.4591 - acc: 0.4688 - val_loss: 1.0133 - val_acc: 0.6378\n",
      "Epoch 00000: val_loss improved from inf to 1.01334, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.9833 - acc: 0.6518 - val_loss: 0.8061 - val_acc: 0.7168\n",
      "Epoch 00001: val_loss improved from 1.01334 to 0.80611, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.8315 - acc: 0.7098 - val_loss: 0.6906 - val_acc: 0.7626\n",
      "Epoch 00002: val_loss improved from 0.80611 to 0.69062, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.7449 - acc: 0.7392 - val_loss: 0.6430 - val_acc: 0.7803\n",
      "Epoch 00003: val_loss improved from 0.69062 to 0.64301, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.6899 - acc: 0.7586 - val_loss: 0.6087 - val_acc: 0.7913\n",
      "Epoch 00004: val_loss improved from 0.64301 to 0.60868, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.6431 - acc: 0.7748 - val_loss: 0.5767 - val_acc: 0.8045\n",
      "Epoch 00005: val_loss improved from 0.60868 to 0.57672, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.6089 - acc: 0.7878 - val_loss: 0.5747 - val_acc: 0.8043\n",
      "Epoch 00006: val_loss improved from 0.57672 to 0.57468, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.5894 - acc: 0.7951 - val_loss: 0.5723 - val_acc: 0.8121\n",
      "Epoch 00007: val_loss improved from 0.57468 to 0.57232, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.5636 - acc: 0.8046 - val_loss: 0.5732 - val_acc: 0.8091\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.5399 - acc: 0.8123 - val_loss: 0.5587 - val_acc: 0.8203\n",
      "Epoch 00009: val_loss improved from 0.57232 to 0.55865, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.5305 - acc: 0.8136 - val_loss: 0.5397 - val_acc: 0.8201\n",
      "Epoch 00010: val_loss improved from 0.55865 to 0.53968, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.5143 - acc: 0.8220 - val_loss: 0.5224 - val_acc: 0.8268\n",
      "Epoch 00011: val_loss improved from 0.53968 to 0.52237, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.5043 - acc: 0.8236 - val_loss: 0.5153 - val_acc: 0.8260\n",
      "Epoch 00012: val_loss improved from 0.52237 to 0.51526, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.4870 - acc: 0.8312 - val_loss: 0.5396 - val_acc: 0.8273\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.4812 - acc: 0.8337 - val_loss: 0.5006 - val_acc: 0.8361\n",
      "Epoch 00014: val_loss improved from 0.51526 to 0.50061, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.4670 - acc: 0.8381 - val_loss: 0.5375 - val_acc: 0.8242\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.4577 - acc: 0.8407 - val_loss: 0.5218 - val_acc: 0.8340\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.4551 - acc: 0.8428 - val_loss: 0.5425 - val_acc: 0.8329\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.4542 - acc: 0.8414 - val_loss: 0.4925 - val_acc: 0.8400\n",
      "Epoch 00018: val_loss improved from 0.50061 to 0.49251, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.4363 - acc: 0.8477 - val_loss: 0.5001 - val_acc: 0.8409\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.4412 - acc: 0.8464 - val_loss: 0.4957 - val_acc: 0.8345\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.4278 - acc: 0.8511 - val_loss: 0.5162 - val_acc: 0.8362\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.4271 - acc: 0.8524 - val_loss: 0.4753 - val_acc: 0.8466\n",
      "Epoch 00022: val_loss improved from 0.49251 to 0.47534, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.4167 - acc: 0.8558 - val_loss: 0.5070 - val_acc: 0.8365\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.4196 - acc: 0.8564 - val_loss: 0.5048 - val_acc: 0.8465\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.4114 - acc: 0.8582 - val_loss: 0.5171 - val_acc: 0.8325\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.4118 - acc: 0.8587 - val_loss: 0.5118 - val_acc: 0.8484\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.4019 - acc: 0.8605 - val_loss: 0.4818 - val_acc: 0.8468\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.4004 - acc: 0.8615 - val_loss: 0.5222 - val_acc: 0.8404\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.4081 - acc: 0.8584 - val_loss: 0.4908 - val_acc: 0.8438\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.3962 - acc: 0.8635 - val_loss: 0.5005 - val_acc: 0.8448\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.3939 - acc: 0.8642 - val_loss: 0.5089 - val_acc: 0.8441\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.3870 - acc: 0.8674 - val_loss: 0.4950 - val_acc: 0.8410\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 27s - loss: 0.3959 - acc: 0.8634 - val_loss: 0.5322 - val_acc: 0.8424\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 00033: early stopping\n"
     ]
    }
   ],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3, 3, border_mode='full'))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Convolution2D(32, 32, 3, 3))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPooling2D(poolsize=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(64, 32, 3, 3, border_mode='full'))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Convolution2D(64, 64, 3, 3))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPooling2D(poolsize=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Convolution2D(128, 64, 3, 3, border_mode='full'))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Convolution2D(128, 128, 3, 3))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPooling2D(poolsize=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128 * (shapex / 2**3) * (shapey / 2**3), 512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512, nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', class_mode='categorical')\n",
    "\n",
    "if not data_augmentation:\n",
    "    print(\"Not using data augmentation or normalization\")\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath='cifar10_cnn_keras_weights.hdf5', verbose=1, save_best_only=True)\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "    X_train = X_train.astype(\"float32\")\n",
    "    X_test = X_test.astype(\"float32\")\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "    model.fit(X_train, Y_train, \n",
    "              batch_size=batch_size, \n",
    "              nb_epoch=nb_epoch, \n",
    "              show_accuracy=True,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              callbacks=[checkpointer, earlystopping])\n",
    "\n",
    "else:\n",
    "    print(\"Using real time data augmentation\")\n",
    "\n",
    "    # this will do preprocessing and realtime data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied)\n",
    "    datagen.fit(X_train)\n",
    "    \n",
    "    current_test_loss = 1000\n",
    "    max_test_loss = 1000\n",
    "    patience_count = 0\n",
    "\n",
    "    for e in range(nb_epoch):\n",
    "        print('-'*40)\n",
    "        print('Epoch', e)\n",
    "        print('-'*40)\n",
    "        print(\"Training...\")\n",
    "        # batch train with realtime data augmentation\n",
    "        progbar = generic_utils.Progbar(X_train.shape[0])\n",
    "        for X_batch, Y_batch in datagen.flow(X_train, Y_train, batch_size=batch_size):\n",
    "            train_res = model.train_on_batch(X_batch, Y_batch, accuracy=True)\n",
    "            progbar.add(X_batch.shape[0], values=[(\"train loss\", train_res[0]), (\"train acc\", train_res[1])])\n",
    "\n",
    "        print(\"Testing...\")\n",
    "        # test time!\n",
    "        progbar = generic_utils.Progbar(X_test.shape[0])\n",
    "        for X_batch, Y_batch in datagen.flow(X_test, Y_test, batch_size=batch_size):\n",
    "            test_res = model.test_on_batch(X_batch, Y_batch, accuracy=True)\n",
    "            current_test_loss = test_res[0]\n",
    "            progbar.add(X_batch.shape[0], values=[(\"test loss\", test_res[0]), (\"test acc\", test_res[1])])\n",
    "            \n",
    "        print(current_test_loss, max_test_loss)\n",
    "        if current_test_loss < max_test_loss:\n",
    "            print('Saving weights to file.')\n",
    "            model.save_weights('cifar10_cnn_keras_weights.hdf5', overwrite=True)\n",
    "            patience_count = 0\n",
    "            max_test_loss = current_test_loss\n",
    "        elif patience_count > 20:\n",
    "            print('\\nEarly stopping.')\n",
    "            break\n",
    "        else:\n",
    "            patience_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "layer_name_dict = {\n",
    "    'Dense': 'denseLayer',\n",
    "    'Dropout': 'dropoutLayer',\n",
    "    'Flatten': 'flattenLayer',\n",
    "    'Embedding': 'embeddingLayer',\n",
    "    'BatchNormalization': 'batchNormalizationLayer',\n",
    "    'LeakyReLU': 'leakyReLULayer',\n",
    "    'PReLU': 'parametricReLULayer',\n",
    "    'ParametricSoftplus': 'parametricSoftplusLayer',\n",
    "    'ThresholdedLinear': 'thresholdedLinearLayer',\n",
    "    'ThresholdedReLu': 'thresholdedReLuLayer',\n",
    "    'LSTM': 'rLSTMLayer',\n",
    "    'GRU': 'rGRULayer',\n",
    "    'JZS1': 'rJZS1Layer',\n",
    "    'JZS2': 'rJZS2Layer',\n",
    "    'JZS3': 'rJZS3Layer',\n",
    "    'Convolution2D': 'convolution2DLayer',\n",
    "    'MaxPooling2D': 'maxPooling2DLayer'\n",
    "}\n",
    "\n",
    "layer_params_dict = {\n",
    "    'Dense': ['weights', 'activation'],\n",
    "    'Dropout': ['p'],\n",
    "    'Flatten': [],\n",
    "    'Embedding': ['weights'],\n",
    "    'BatchNormalization': ['weights', 'epsilon'],\n",
    "    'LeakyReLU': ['alpha'],\n",
    "    'PReLU': ['weights'],\n",
    "    'ParametricSoftplus': ['weights'],\n",
    "    'ThresholdedLinear': ['theta'],\n",
    "    'ThresholdedReLu': ['theta'],\n",
    "    'LSTM': ['weights', 'activation', 'inner_activation'],\n",
    "    'GRU': ['weights', 'activation', 'inner_activation'],\n",
    "    'JZS1': ['weights', 'activation', 'inner_activation'],\n",
    "    'JZS2': ['weights', 'activation', 'inner_activation'],\n",
    "    'JZS3': ['weights', 'activation', 'inner_activation'],\n",
    "    'Convolution2D': ['weights', 'nb_filter', 'stack_size', 'nb_row', 'nb_col', 'border_mode', 'subsample', 'activation'],\n",
    "    'MaxPooling2D': ['poolsize', 'stride', 'ignore_border']\n",
    "}\n",
    "\n",
    "layer_weights_dict = {\n",
    "    'Dense': ['W', 'b'],\n",
    "    'Embedding': ['E'],\n",
    "    'BatchNormalization': ['gamma', 'beta', 'mean', 'std'],\n",
    "    'PReLU': ['alphas'],\n",
    "    'ParametricSoftplus': ['alphas', 'betas'],\n",
    "    'LSTM': ['W_xi', 'W_hi', 'b_i', 'W_xc', 'W_hc', 'b_c', 'W_xf', 'W_hf', 'b_f', 'W_xo', 'W_ho', 'b_o'],\n",
    "    'GRU': ['W_xz', 'W_hz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h'],\n",
    "    'JZS1': ['W_xz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_hh', 'b_h', 'Pmat'],\n",
    "    'JZS2': ['W_xz', 'W_hz', 'b_z', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h', 'Pmat'],\n",
    "    'JZS3': ['W_xz', 'W_hz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h'],\n",
    "    'Convolution2D': ['W', 'b']\n",
    "}\n",
    "\n",
    "def serialize(model_json_file, weights_hdf5_file, save_filepath, compress):\n",
    "    with open(model_json_file, 'r') as f:\n",
    "        model_metadata = json.load(f)\n",
    "    weights_file = h5py.File(weights_hdf5_file, 'r')\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    num_activation_layers = 0\n",
    "    for k, layer in enumerate(model_metadata['layers']):\n",
    "        if layer['name'] == 'Activation':\n",
    "            num_activation_layers += 1\n",
    "            prev_layer_name = model_metadata['layers'][k-1]['name']\n",
    "            idx_activation = layer_params_dict[prev_layer_name].index('activation')\n",
    "            layers[k-num_activation_layers]['parameters'][idx_activation] = layer['activation']\n",
    "            continue\n",
    "\n",
    "        layer_params = []\n",
    "\n",
    "        for param in layer_params_dict[layer['name']]:\n",
    "            if param == 'weights':\n",
    "                layer_weights = list(weights_file.get('layer_{}'.format(k)))\n",
    "                weights = {}\n",
    "                weight_names = layer_weights_dict[layer['name']]\n",
    "                for name, w in zip(weight_names, layer_weights):\n",
    "                    weights[name] = weights_file.get('layer_{}/{}'.format(k, w)).value.tolist()\n",
    "                layer_params.append(weights)\n",
    "            else:\n",
    "                layer_params.append(layer[param])\n",
    "\n",
    "        layers.append({\n",
    "            'layerName': layer_name_dict[layer['name']],\n",
    "            'parameters': layer_params\n",
    "        })\n",
    "\n",
    "    if compress:\n",
    "        with gzip.open(save_filepath, 'wb') as f:\n",
    "            f.write(json.dumps(layers).encode('utf8'))\n",
    "    else:\n",
    "        with open(save_filepath, 'w') as f:\n",
    "            json.dump(layers, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "model_metadata = json.loads(model.to_json())\n",
    "\n",
    "with open('cifar10_cnn_keras_model.json', 'w') as f:\n",
    "    json.dump(model_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "serialize('cifar10_cnn_keras_model.json', \n",
    "          'cifar10_cnn_keras_weights.hdf5', \n",
    "          'cifar10_cnn_model_params.json.gz', \n",
    "          True)\n",
    "serialize('cifar10_cnn_keras_model.json', \n",
    "          'cifar10_cnn_keras_weights.hdf5', \n",
    "          'cifar10_cnn_model_params.json', \n",
    "          False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "randidx = np.random.randint(0, X_test.shape[0], size=500)\n",
    "X_rand = X_test[randidx, :]\n",
    "y_rand = y_test[randidx]\n",
    "\n",
    "with gzip.open('sample_data.json.gz', 'wb') as f:\n",
    "    f.write(json.dumps({'data': X_rand.tolist(), 'labels': y_rand.tolist()}).encode('utf8'))\n",
    "with open('sample_data.json', 'w') as f:\n",
    "    json.dump({'data': X_rand.tolist(), 'labels': y_rand.tolist()}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.77 ms, sys: 721 Âµs, total: 4.49 ms\n",
      "Wall time: 4.07 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.14086735e-16,   1.13544659e-20,   3.05306025e-09,\n",
       "          1.41454004e-08,   6.85371488e-05,   2.38421781e-05,\n",
       "          2.99920073e-15,   9.99907613e-01,   3.84884120e-18,\n",
       "          2.40887647e-16]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.predict(X_rand[0:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
